# ğŸš€ Quick Reference Card

## One-Command Start

```bash
# Terminal 1: Start Ollama
ollama serve

# Terminal 2: Start RAG System
npm run dev
```

## Essential URLs

| Interface | URL |
|-----------|-----|
| ğŸ’¬ Chat UI | http://localhost:3000 |
| ğŸ“š API Docs | http://localhost:3000/api-docs |
| ğŸ“Š Status | http://localhost:3000/api/status |

## Common Commands

```bash
# Development
npm run dev              # Start with hot reload
npm run build            # Compile TypeScript
npm start                # Run production server

# MCP Server
npm run mcp              # Development MCP server
npm run mcp:prod         # Production MCP server (after build)

# Utilities
npm run clean            # Clean build files
```

## API Quick Test

```bash
# Status
curl http://localhost:3000/api/status

# Retrieve
curl -X POST http://localhost:3000/api/retrieve \
  -H "Content-Type: application/json" \
  -d '{"query":"test","limit":3}'

# Generate
curl -X POST http://localhost:3000/api/generate \
  -H "Content-Type: application/json" \
  -d '{"query":"What is this?"}'

# Stream
curl -X POST http://localhost:3000/api/chat/stream \
  -H "Content-Type: application/json" \
  -d '{"query":"Hello"}'

# Reindex
curl -X POST http://localhost:3000/api/status/reindex
```

## File Locations

```
ğŸ“ Documents to index â†’ ./docs/*.md
ğŸ“ Logs              â†’ ./logs/combined.log
ğŸ“ Vector DB         â†’ ./data/lancedb/
ğŸ“ Web interface     â†’ ./public/
ğŸ“ Configuration     â†’ .env
```

## Environment Variables

```env
# Core Settings
PORT=3000                                    # Server port
MODEL=llama3                                 # Ollama model
OLLAMA_BASE_URL=http://localhost:11434     # Ollama URL
EMBEDDING_MODEL=Xenova/bge-base-en-v1.5    # Embedding model

# RAG Settings
TOP_K_RESULTS=5        # Results to retrieve
CHUNK_SIZE=1000        # Document chunk size
CHUNK_OVERLAP=200      # Chunk overlap

# Advanced
LOG_LEVEL=info         # Logging level (debug, info, warn, error)
NODE_ENV=development   # Environment
```

## MCP Setup (Cursor)

**Config file:** `~/.config/cursor/mcp.json`

```json
{
  "mcpServers": {
    "rn-firebase-chat-rag": {
      "command": "npm",
      "args": ["run", "mcp:prod"],
      "cwd": "/Users/tungle/saigontechnology/rn-firebase-chat-rag"
    }
  }
}
```

**Steps:**
1. `npm run build`
2. Edit config (use your actual path!)
3. Restart Cursor
4. Test: "Use RAG retrieve tool to find X"

## Troubleshooting

| Problem | Solution |
|---------|----------|
| Can't connect to Ollama | `ollama serve` in another terminal |
| No documents found | Add `.md` files to `./docs/` |
| Port in use | Change `PORT` in `.env` |
| Model not found | `ollama pull llama3` |
| TypeScript errors | `npm install` |

## Project Structure

```
src/
â”œâ”€â”€ index.ts           # Main server
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ pipeline.ts    # RAG orchestrator
â”‚   â”œâ”€â”€ embedder.ts    # Embeddings
â”‚   â”œâ”€â”€ vectorStore.ts # Vector DB
â”‚   â”œâ”€â”€ loader.ts      # Document loader
â”‚   â”œâ”€â”€ retriever.ts   # Search
â”‚   â””â”€â”€ generator.ts   # LLM
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ retrieve.ts    # /api/retrieve
â”‚   â”œâ”€â”€ generate.ts    # /api/generate
â”‚   â”œâ”€â”€ chat.ts        # /api/chat/stream
â”‚   â””â”€â”€ status.ts      # /api/status
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ logger.ts      # Winston
â”‚   â”œâ”€â”€ modelConfig.ts # Config
â”‚   â””â”€â”€ swagger.ts     # API docs
â””â”€â”€ mcp/
    â””â”€â”€ server.ts      # MCP server
```

## Component Flow

```
User Query
    â†“
Embedder (convert to vector)
    â†“
VectorStore (similarity search)
    â†“
Retriever (get top-k docs)
    â†“
Generator (LLM with context)
    â†“
Response
```

## Key Features

- âœ… Local LLM (no API keys needed)
- âœ… Semantic search with embeddings
- âœ… Streaming responses
- âœ… REST API + Web UI + MCP
- âœ… TypeScript strict mode
- âœ… Production ready

## Useful Tips

1. **First time slow?** Embedding model downloads (~100MB)
2. **Add documents:** Copy `.md` files to `./docs/`
3. **Reindex:** Call `/api/status/reindex` or restart
4. **Check logs:** `tail -f logs/combined.log`
5. **Test API:** Use Swagger UI at `/api-docs`
6. **Change model:** Edit `MODEL` in `.env`

## Performance Tips

- Use smaller models for faster responses
- Reduce `TOP_K_RESULTS` for faster retrieval
- Increase `CHUNK_SIZE` for fewer chunks
- Clear data directory to start fresh

## Development Tips

```bash
# Watch logs in real-time
tail -f logs/combined.log

# Clear vector database
rm -rf data/lancedb

# Test with curl
alias rag-status='curl http://localhost:3000/api/status'
alias rag-retrieve='curl -X POST http://localhost:3000/api/retrieve -H "Content-Type: application/json" -d'

# Use with alias
rag-retrieve '{"query":"test"}'
```

## Example Questions

Once documents are indexed, try:
- "What is this system about?"
- "How do I get started?"
- "What are the main features?"
- "Explain the architecture"

## Production Checklist

- [ ] Build: `npm run build`
- [ ] Test all endpoints
- [ ] Configure `.env` for production
- [ ] Set `NODE_ENV=production`
- [ ] Use process manager (PM2)
- [ ] Set up reverse proxy
- [ ] Configure SSL/TLS
- [ ] Set up monitoring
- [ ] Configure firewall

## Support Files

- ğŸ“– `README.md` - Complete documentation
- ğŸš€ `SETUP.md` - Step-by-step setup
- ğŸ”Œ `src/mcp/README.md` - MCP guide
- ğŸ“Š `PROJECT_SUMMARY.md` - What was built
- âš¡ `QUICK_REFERENCE.md` - This file

## Need Help?

1. Check README.md
2. Review logs
3. Test REST API first
4. Verify Ollama is running
5. Check documents are in `./docs/`

---

**Remember:** Start Ollama first, then the RAG system!

```bash
# Terminal 1
ollama serve

# Terminal 2
npm run dev

# Browser
http://localhost:3000
```

**You're all set! ğŸ‰**

